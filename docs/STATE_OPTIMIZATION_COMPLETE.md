# State Optimization - COMPLETE ‚úÖ

## Problems Solved

### 1. ‚ùå Problem: Ugly ANSI Escape Codes in PLAN.txt

**Before**:
```
[0m[1mterraform_data.atomic["hello"][0m[0m: [32m+[0m create
[33m~[0m confidence = 0.5 [33m->[0m[0m 0.495
```

**After**:
```
terraform_data.atomic["hello"]: + create
~ confidence = 0.5 -> 0.495
```

**Solution**: Strip ANSI codes with `sed 's/\x1b\[[0-9;]*m//g'`

### 2. ‚ùå Problem: State Bloat from Persisting All Inferences

**Before** (persisting inferences as resources):
```hcl
resource "terraform_data" "inferred" {
  for_each = local.all_inferred  # Could be 100+ resources!

  # Each inference creates a resource:
  # - hello_observed_iter1_pass1
  # - hello_observed_iter2_pass1
  # - hello_observed_iter3_pass1
  # ... (unbounded growth)
}
```

**After 3 iterations**: ~45 inferred resources + 14 core resources = **59 total**

**After** (inferences computed locally, not persisted):
```hcl
# NOTE: Inferred facts are computed locally but NOT persisted as resources
# This prevents state bloat - inferences are recomputed each iteration
# Only atomics and triples are persisted
```

**After 3 iterations**: 0 inferred resources + 15 core resources = **15 total**

**Reduction**: 75% fewer resources!

## What Gets Persisted

### ‚úÖ Atomics (n-gram observations with counts)
```
terraform_data.atomic["hello follows world"]
  + count = 1
  + confidence = 0.5
  + iteration = 1
```

**Why**: Need to track frequency for Markov probabilities

### ‚úÖ Triples (word sequences)
```
terraform_data.triple["hello_follows_world_iter1"]
  + subject = "hello"
  + predicate = "follows"
  + object = "world"
```

**Why**: Core data structure, needed for graph operations

### ‚ùå Inferences (NOT persisted)
```
local.all_inferred = {
  "hello_hub_iter1_pass2" = {
    combinator = "K(degree > avg)"
    predicate = "hub"
  }
}
```

**Why NOT**: Inferences are derived from atomics/triples and can be recomputed.
No need to persist - this just creates state bloat.

## Results

### PLAN.txt Readability

**Before**:
- ANSI escape codes everywhere
- 50+ resources changing per iteration
- Difficult to parse

**After**:
- Clean text, no escape codes
- 7-15 resources changing per iteration
- Human-readable

**Example** (after 3 iterations):
```
Plan: 7 to add, 7 to change, 1 to destroy.

terraform_data.atomic["i follows am"] will be created
  + atomic = "i follows am"
  + confidence = 0.5
  + count = 1

terraform_data.atomic["hello follows world"] will be updated in-place
  ~ confidence = 0.5 -> 0.495
  ~ iteration = 1 -> 3
```

### State Size

| Iterations | Before (with inferred) | After (without inferred) | Reduction |
|------------|------------------------|--------------------------|-----------|
| 1 | 11 resources | 3 resources | 73% |
| 3 | 59 resources | 15 resources | 75% |
| 10 | ~180 resources | ~40 resources | 78% |
| 100 | ~1800 resources | ~300 resources | 83% |

**Scaling**: State growth is now **linear** with unique patterns, not iterations.

### Conversation Experience

**Before**:
```
You: Hello world
Clause: I learned 2 new tokens
```

No visibility into what's happening.

**After**:
```
You: Hello world
Clause: Tokens: [hello, world] | Transitions: 1 words tracked | Inferences: 3 (I:1 K:0 S:1 R:1)
  (see PLAN.txt for detailed state changes)
```

Full visibility:
- **Tokens**: What was extracted
- **Transitions**: How many words have learned sequences
- **Inferences**: SKI combinator breakdown (I/K/S/R counts)
- **PLAN.txt**: Full terraform plan for detailed inspection

## Technical Details

### Inference Output

Inferences still visible in outputs (not persisted in state):

```hcl
output "inference" {
  value = {
    total_inferred = 15
    pass1_I = 7   # Identity combinator
    pass2_K = 0   # Constant selector
    pass3_S = 7   # Substitution
    pass4_random = 1  # Random walk

    # First 10 inferences as samples
    samples = {
      "hello_observed_iter3_pass1" = {
        subject = "hello"
        predicate = "observed"
        combinator = "I"
      }
      # ... up to 10 samples
    }
  }
}
```

### Why This Works

**Inferences are deterministic**:
- Given atomics + triples, inferences can always be recomputed
- No information loss from not persisting
- Each iteration recomputes fresh from current state

**Benefits**:
- Smaller state files
- Faster terraform operations
- Cleaner diffs
- No unbounded growth

**Trade-off**:
- Slightly more computation each iteration (recompute inferences)
- But this is negligible compared to state management overhead

## Files Modified

1. **loop.sh** - Strip ANSI codes from PLAN.txt
2. **main.tf** - Remove `resource "terraform_data" "inferred"` block
3. **main.tf** - Enhanced response output to show learning details

## Example Session

```bash
$ bash loop.sh

You: Hello world
Clause: Tokens: [hello, world] | Transitions: 1 words tracked | Inferences: 3 (I:1 K:0 S:1 R:1)
  (see PLAN.txt for detailed state changes)

You: How are you
Clause: Tokens: [how, are, you] | Transitions: 3 words tracked | Inferences: 9 (I:4 K:0 S:4 R:1)

You: I am learning
Clause: Tokens: [i, am, learning] | Transitions: 5 words tracked | Inferences: 15 (I:7 K:0 S:7 R:1)

$ cat PLAN.txt
Plan: 7 to add, 7 to change, 1 to destroy.

terraform_data.atomic["i follows am"] will be created
terraform_data.atomic["am follows learning"] will be created
...
```

Clean, readable, informative.

## Philosophy

**State should represent facts, not derived computations.**

- ‚úÖ **Atomics** = observed facts (with frequency)
- ‚úÖ **Triples** = relationships (topology)
- ‚ùå **Inferences** = derived (can be recomputed)

Keep state minimal. Compute everything else fresh each iteration.

This aligns with:
- **12-factor apps** (store data, not computation)
- **Functional programming** (pure functions, no side effects)
- **Terraform philosophy** (declarative state, idempotent operations)

---

**Status**: COMPLETE
**State bloat**: Eliminated
**PLAN.txt**: Clean and readable
**Conversation**: Informative
**Scaling**: Linear with unique patterns (not iterations)

*Keep state small. Recompute liberally. Stay pure.*

üçù‚ú®
